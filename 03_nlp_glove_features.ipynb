{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1971bf34-2219-4c81-a52e-d973020a123e",
   "metadata": {},
   "source": [
    "<h2> 3.6 Featurizing text data with tfidf weighted word-vectors </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64b6831-10b2-415b-a7ae-63efe2729207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# exctract word2vec vectors\n",
    "# https://github.com/explosion/spaCy/issues/1721\n",
    "# http://landinghub.visualstudio.com/visual-cpp-build-tools\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcceb89-187b-4a4a-aa1e-4a3e88d96ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85134839-32a5-496d-abb8-490bc400a9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avoid decoding problems\n",
    "df_data = pd.read_csv(\"data/00_train.csv\")\n",
    "\n",
    "df_data['question1'] = df_data['question1'].apply(lambda x: str(x))\n",
    "df_data['question2'] = df_data['question2'].apply(lambda x: str(x))\n",
    "df_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb06221-942c-448b-bb88-3c90facf21bf",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Applying TFIDF WEIGHT W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece43fe4-1c08-4140-99ef-2063f6637810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "questions = list(df_data['question1'][:data_points] + df_data['question2'][:data_points])\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, )\n",
    "tfidf.fit_transform(questions)\n",
    "\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf8dabb-6e35-4145-8c38-d7d8b9563c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000   6.704115752446321\n",
      "09   8.313553664880422\n",
      "10   5.540964942640641\n",
      "100   6.809476268104148\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for k, v in word2tfidf.items():\n",
    "    if counter == 4:\n",
    "        break\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(k, ' ', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584aa984-bb3b-49d6-921e-efa49987ed51",
   "metadata": {},
   "source": [
    "- After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.\n",
    "- here we use a pre-trained GLOVE model which comes free with \"Spacy\".  https://spacy.io/usage/vectors-similarity\n",
    "- It is trained on Wikipedia and therefore, it is stronger in terms of word semantics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7c84db-6f86-4d93-aa4b-92fb8c9694bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vectors_web_lg, which includes over 1 million unique vectors.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tifidf_to_wieghtedW2V(text_df, tfidf_vectors):\n",
    "    vecs = []\n",
    "    # https://github.com/noamraph/tqdm\n",
    "    # tqdm is used to print the progress bar\n",
    "    for ques in tqdm(list(text_df)):\n",
    "        doc = nlp(ques) \n",
    "        # 384 is the number of dimensions of vectors \n",
    "        mean_vec = np.zeros([len(doc), len(doc[0].vector)])\n",
    "        for word in doc:\n",
    "            # word2vec\n",
    "            vec = word.vector\n",
    "            # fetch df score\n",
    "            try:\n",
    "                idf = tfidf_vectors[str(word)]\n",
    "            except:\n",
    "                idf = 0\n",
    "            # compute final vec\n",
    "            mean_vec += vec * idf\n",
    "        mean_vec = mean_vec.mean(axis=0)\n",
    "        vecs.append(mean_vec)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308ef930-d81f-4723-ab00-da0903b1201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:31<00:00, 95.93it/s]\n"
     ]
    }
   ],
   "source": [
    "q1 = list(tifidf_to_wieghtedW2V(df_data['question1'][:data_points], word2tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace9c1ca-d64f-4289-b42e-0d4112e768ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/01_nlp/03_1_tfidf_weighted_w2v_3000.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(q1, f'models/01_nlp/03_1_tfidf_weighted_w2v_{data_points}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc29e036-a561-4bca-9832-4b18a921e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:30<00:00, 99.66it/s]\n"
     ]
    }
   ],
   "source": [
    "q2 = list(tifidf_to_wieghtedW2V(df_data['question2'][:data_points], word2tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3d9aaa-870a-427b-8c56-ee5c8a4010d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/01_nlp/04_2_tfidf_weighted_w2v3000.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(q2, f'models/01_nlp/04_2_tfidf_weighted_w2v{data_points}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab0865-0fd3-411f-9fa6-b9d4c6807eb4",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Applying Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb96b95-0858-4f45-9fa5-4e32acff46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0803164a-a025-47c6-9424-e1272c8f20c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1917494, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove2word2vec(glove_input_file=\"data/glove.42B.300d.txt\", word2vec_output_file=\"data/glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a491e57-b664-473d-83fa-c991debb5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m gensim.scripts.glove2word2vec --input  data/glove.840B.300d.txt --output data/glove.840B.300d.w2vformat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133d8bad-0d54-416f-a503-b56ada056732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"data/glove_vectors.txt\", binary=False, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c61eb5-3e74-4e77-aee8-46f9d46b98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(s1, s2,model):\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6416e-48a7-48cb-a4ca-53fc4b47b373",
   "metadata": {},
   "source": [
    "http://proceedings.mlr.press/v37/kusnerb15.pdf i read about word mover distance and after that i calculated some distances from avg word vectors as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6edd58-b0ee-413c-938b-7bd9927cb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data \n",
    "df['Word_Mover_Dist'] = df.apply(lambda x: wmd(x['question1'], x['question2'],glove_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f992f5-364f-42e1-9851-167276fade35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the avg-w2v for each sentence/review is stored in this list\n",
    "def avg_w2v(list_of_sent,model,d):\n",
    "    '''\n",
    "    Returns average of word vectors for\n",
    "    each sentance with dimension of model given\n",
    "    '''\n",
    "    sent_vectors = []\n",
    "    for sent in list_of_sent: # for each review/sentence\n",
    "        doc = [word for word in sent if word in model.key_to_index]\n",
    "        if doc: \n",
    "            sent_vec = np.mean(model[doc],axis=0)\n",
    "        else:\n",
    "            sent_vec = np.zeros(d)\n",
    "        sent_vectors.append(sent_vec)\n",
    "    return sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693353b-38f5-445f-8c20-06a78c357c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into lists\n",
    "list_of_question1=[]\n",
    "for sent in df.question1.values:\n",
    "    list_of_question1.append(sent.split())\n",
    "list_of_question2=[]\n",
    "for sent in df.question2.values:\n",
    "    list_of_question2.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e7abe-1e86-47c2-b32a-9fdb3ab24010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg word 2 vec\n",
    "# d= [word for word in sent if word in glove_model.key_to_index]\n",
    "# sent_vec = np.mean(glove_model[d],axis=0)\n",
    "\n",
    "avgw2v_q1 = avg_w2v(list_of_question1,glove_model,300)\n",
    "avgw2v_q2 = avg_w2v(list_of_question2,glove_model,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b28a0eaa-957f-4eb8-8a8b-5f347c894a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting as df\n",
    "df_avgw2v = pd.DataFrame()\n",
    "df_avgw2v['q1_vec'] = list(avgw2v_q1)\n",
    "df_avgw2v['q2_vec'] = list(avgw2v_q2)\n",
    "df_q1 = pd.DataFrame(df_avgw2v.q1_vec.values.tolist())\n",
    "df_q2 = pd.DataFrame(df_avgw2v.q2_vec.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d5ac6-f0fe-4a24-b4c7-881723bae7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dde7a466-1a05-4146-b0e1-3a0c878172f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing soma distances and calculating\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock,canberra, euclidean, minkowski\n",
    "from scipy.spatial.distance import braycurtis, chebyshev, correlation, mahalanobis\n",
    "from scipy.spatial.distance import seuclidean, hamming, jaccard, kulsinski, rogerstanimoto, russellrao, sokalmichener, sokalsneath, kulsinski, yule\n",
    "\n",
    "df['dist_cosine'] = [cosine(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_cityblock'] = [cityblock(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_canberra'] = [canberra(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_euclidean'] = [euclidean(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_minkowski'] = [minkowski(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_braycurtis'] = [braycurtis(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_chebyshev'] = [chebyshev(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_correlation'] = [correlation(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_hamming'] = [hamming(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_jaccard'] = [jaccard(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_kulsinski'] = [kulsinski(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_rogerstanimoto'] = [rogerstanimoto(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_russellrao'] = [russellrao(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_sokalmichener'] = [sokalmichener(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_kulsinski'] = [kulsinski(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "df['dist_yule'] = [yule(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3246e86c-51e3-4ba8-9d05-e2da848bce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling na values with 0  for cosine distance\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19fe5338-9ece-47c9-b901-62d870219682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>Word_Mover_Dist</th>\n",
       "      <th>dist_cosine</th>\n",
       "      <th>dist_cityblock</th>\n",
       "      <th>dist_canberra</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_braycurtis</th>\n",
       "      <th>dist_chebyshev</th>\n",
       "      <th>dist_correlation</th>\n",
       "      <th>dist_hamming</th>\n",
       "      <th>dist_jaccard</th>\n",
       "      <th>dist_kulsinski</th>\n",
       "      <th>dist_rogerstanimoto</th>\n",
       "      <th>dist_russellrao</th>\n",
       "      <th>dist_sokalmichener</th>\n",
       "      <th>dist_yule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.144728</td>\n",
       "      <td>0.006854</td>\n",
       "      <td>9.08724</td>\n",
       "      <td>82.744686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117393</td>\n",
       "      <td>0.112668</td>\n",
       "      <td>0.00684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860943</td>\n",
       "      <td>-0.670791</td>\n",
       "      <td>0.895868</td>\n",
       "      <td>-0.670791</td>\n",
       "      <td>0.233231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "\n",
       "   Word_Mover_Dist  dist_cosine  dist_cityblock  dist_canberra  ...  \\\n",
       "0         0.144728     0.006854         9.08724      82.744686  ...   \n",
       "\n",
       "   dist_braycurtis  dist_chebyshev  dist_correlation  dist_hamming  \\\n",
       "0         0.117393        0.112668           0.00684           1.0   \n",
       "\n",
       "   dist_jaccard  dist_kulsinski  dist_rogerstanimoto  dist_russellrao  \\\n",
       "0           1.0        0.860943            -0.670791         0.895868   \n",
       "\n",
       "   dist_sokalmichener  dist_yule  \n",
       "0           -0.670791   0.233231  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbf12268-1392-4b66-b701-e6ee9c88f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/03_nlp_glove_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c1a555-3477-4c1f-84c7-f30306649b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = df_data['question1'][:2000]\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10, lowercase=False)\n",
    "tfidf_1 = tfidf_vect.fit_transform(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63f3e1-0867-494a-ab44-933f79bbbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tfidf_1, \"models/01_nlp/01_1_tfidf_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c915352-a475-4bca-ba7e-945314ed8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = df_data['question2'][:2000]\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10, lowercase=False)\n",
    "tfidf_2 = tfidf_vect.fit_transform(question2)\n",
    "type(tfidf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5332fd6-14e3-4ef0-8044-e5f4deb14beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tfidf_2, \"models/01_nlp/02_2_tfidf_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0424d-a8fd-4fe9-b40a-64757cd12ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a63c50-dc26-416b-b780-a7f63adbaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,3),max_features=200000,min_df=0.000032)\n",
    "train_tfidf = tfidf_vect.fit_transform(X_train_tf.Text)\n",
    "test_tfidf = tfidf_vect.transform(X_test_tf.Text)\n",
    "print('No of Tfidf features',len(tfidf_vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466dd60-18e7-483f-92a7-29e6912cb08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train1 = hstack((X_train_tf.values,train_tfidf))\n",
    "X_test1 = hstack((X_test_tf.values,test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b148f11-f589-4168-803b-598687b7c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler(with_mean=False)\n",
    "X_train_sc = scale.fit_transform(X_train1)\n",
    "X_test_sc = scale.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b557bf6-729c-48a3-95c8-d593f379d773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450e1e7-ce0a-4e6d-b96f-fe8c8d52adb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d47076-462a-4bbd-a04e-01b512af1ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198c872-36da-44d6-aca7-74e786c4b0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00b4a7-66f9-4978-88de-0f27e6616986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507fa41-1fbf-4ae0-9e85-e7f620b7ab57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5fb7e-40c9-4d37-bee7-d6835f67f286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50859410-615a-4a56-8077-d443d9e193dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb928d-b9eb-4afb-a834-562e10440d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
