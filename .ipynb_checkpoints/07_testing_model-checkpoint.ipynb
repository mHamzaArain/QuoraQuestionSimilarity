{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0b2b07-cfe3-47b3-a677-228ef03b1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#importing soma distances and calculating\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock,canberra, euclidean, minkowski\n",
    "from scipy.spatial.distance import braycurtis, chebyshev, correlation, mahalanobis\n",
    "from scipy.spatial.distance import seuclidean, hamming, jaccard, kulsinski, rogerstanimoto,\\\n",
    "                        russellrao, sokalmichener, sokalsneath, kulsinski, yule\n",
    "\n",
    "import spacy\n",
    "# en_vectors_web_lg, which includes over 1 million unique vectors.\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6b5e26-4a32-49c6-8ee2-b8f9f410c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadEssentials():\n",
    "    def __init__(self, glove_file, word2tfidf_file, model_file):\n",
    "        self.glove_file = glove_file\n",
    "        self.word2tfidf_file= word2tfidf_file\n",
    "        self.model_file = model_file\n",
    "        \n",
    "    def load_all(self):\n",
    "        glove_model = self.load_glove_model(self.glove_file)\n",
    "        word2tfidf = self.load_word2tfidf(self.word2tfidf_file ,3000)\n",
    "        model = self.load_ml_model(self.model_file)\n",
    "        return glove_model, word2tfidf, model\n",
    "    \n",
    "    def load_glove_model(self, file):\n",
    "        return KeyedVectors.load_word2vec_format(file, binary=False, unicode_errors='ignore')\n",
    "    \n",
    "    def load_word2tfidf(self, file ,data_points):\n",
    "        df = pd.read_csv(file)\n",
    "        df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "        df['question2'] = df['question2'].apply(lambda x: str(x))\n",
    "        questions = list(df['question1'] + df['question2'])\n",
    "\n",
    "        tfidf = TfidfVectorizer(lowercase=False, )\n",
    "        tfidf.fit_transform(questions)\n",
    "\n",
    "        # dict key:word and value:tf-idf score\n",
    "        return dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "        \n",
    "    def load_ml_model(self, file):\n",
    "        return joblib.load(file)\n",
    "    \n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "\n",
    "class BasicFeature():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def extract_features(self):\n",
    "        self.df['freq_qid1'] = self.df.groupby('qid1')['qid1'].transform('count') \n",
    "        self.df['freq_qid2'] = self.df.groupby('qid2')['qid2'].transform('count')\n",
    "        self.df['q1len'] = self.df['question1'].str.len() \n",
    "        self.df['q2len'] = self.df['question2'].str.len()\n",
    "        self.df['q1_n_words'] = self.df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "        self.df['q2_n_words'] = self.df['question2'].apply(lambda row: len(row.split(\" \")))        \n",
    "        self.df['word_Common'] = self.df.apply(self.normalized_word_Common, axis=1)\n",
    "        self.df['word_Total'] = self.df.apply(self.normalized_word_Total, axis=1)\n",
    "        self.df['word_share'] = self.df.apply(self.normalized_word_share, axis=1)\n",
    "        self.df['freq_q1+q2'] = self.df['freq_qid1']+self.df['freq_qid2']\n",
    "        self.df['freq_q1-q2'] = abs(self.df['freq_qid1']-self.df['freq_qid2'])\n",
    "        return self.df\n",
    "        \n",
    "    def normalized_word_Common(self, row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)\n",
    "\n",
    "    def normalized_word_Total(self, row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * (len(w1) + len(w2))\n",
    "\n",
    "    def normalized_word_share(self, row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "    \n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "\n",
    "class AdvanceFeature():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # To get the results in 4 decemal points\n",
    "        self.SAFE_DIV = 0.0001 \n",
    "        self.STOP_WORDS = stopwords.words(\"english\")\n",
    "        \n",
    "    def extract_features(self):\n",
    "        # preprocessing each question\n",
    "        self.df[\"question1\"] = self.df[\"question1\"].fillna(\"\").apply(self.preprocess)\n",
    "        self.df[\"question2\"] = self.df[\"question2\"].fillna(\"\").apply(self.preprocess)\n",
    "\n",
    "        # Merging Features with dataset\n",
    "        token_features = self.df.apply(lambda x: self.get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "        self.df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "        self.df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "        self.df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "        self.df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "        self.df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "        self.df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "        self.df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "        self.df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "        self.df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "        self.df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "\n",
    "        #Computing Fuzzy Features and Merging with Dataset\n",
    "\n",
    "        # do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "        # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n",
    "        # https://github.com/seatgeek/fuzzywuzzy\n",
    "        self.df[\"token_set_ratio\"] = self.df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "        # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n",
    "        # then joining them back into a string We then compare the transformed strings with a simple ratio().\n",
    "        self.df[\"token_sort_ratio\"] = self.df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "        self.df[\"fuzz_ratio\"] = self.df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "        self.df[\"fuzz_partial_ratio\"] = self.df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "        return self.df\n",
    "        \n",
    "    def get_token_features(self, q1, q2):\n",
    "        token_features = [0.0]*10\n",
    "        \n",
    "        # Converting the Sentence into Tokens: \n",
    "        q1_tokens = q1.split()\n",
    "        q2_tokens = q2.split()\n",
    "\n",
    "        if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "            return token_features\n",
    "        \n",
    "        # Get the non-stopwords in Questions\n",
    "        q1_words = set([word for word in q1_tokens if word not in self.STOP_WORDS])\n",
    "        q2_words = set([word for word in q2_tokens if word not in self.STOP_WORDS])\n",
    "\n",
    "        #Get the stopwords in Questions\n",
    "        q1_stops = set([word for word in q1_tokens if word in self.STOP_WORDS])\n",
    "        q2_stops = set([word for word in q2_tokens if word in self.STOP_WORDS])\n",
    "        # Get the common non-stopwords from Question pair\n",
    "        common_word_count = len(q1_words.intersection(q2_words))\n",
    "        # Get the common stopwords from Question pair\n",
    "        common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "\n",
    "        # Get the common Tokens from Question pair\n",
    "        common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "        token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + self.SAFE_DIV)\n",
    "        token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + self.SAFE_DIV)\n",
    "        token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + self.SAFE_DIV)\n",
    "        token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + self.SAFE_DIV)\n",
    "        token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + self.SAFE_DIV)\n",
    "        token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + self.SAFE_DIV)\n",
    "        # Last word of both question is same or not\n",
    "        token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "        # First word of both question is same or not\n",
    "        token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "        token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "        #Average Token Length of both Questions\n",
    "        token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "        return token_features\n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        x = str(x).lower()\n",
    "        x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "        x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "        x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "\n",
    "        porter = PorterStemmer()\n",
    "        pattern = re.compile('\\W')\n",
    "        \n",
    "        if type(x) == type(''):\n",
    "            x = re.sub(pattern, ' ', x)\n",
    "        if type(x) == type(''):\n",
    "            x = porter.stem(x)\n",
    "            example1 = BeautifulSoup(x)\n",
    "            x = example1.get_text()\n",
    "        return x\n",
    "    \n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "\n",
    "class DistanceFeature():\n",
    "    def __init__(self, df, glove):\n",
    "        self.df = df\n",
    "        self.glove_model = glove\n",
    "        \n",
    "    def extract_features(self):\n",
    "        #converting into lists\n",
    "        list_of_question1=[]\n",
    "        for sent in df['question1'].values:\n",
    "            list_of_question1.append(sent.split())\n",
    "        list_of_question2=[]\n",
    "        for sent in df['question2'].values:\n",
    "            list_of_question2.append(sent.split())\n",
    "\n",
    "        avgw2v_q1 = self.avg_w2v(list_of_question1, self.glove_model,300)\n",
    "        avgw2v_q2 = self.avg_w2v(list_of_question2, self.glove_model,300)\n",
    "        self.df['Word_Mover_Dist'] = self.df.apply(lambda x: self.wmd(x['question1'], x['question2'], self.glove_model), axis=1)\n",
    "        self.df['dist_cosine'] = [cosine(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_cityblock'] = [cityblock(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_canberra'] = [canberra(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_euclidean'] = [euclidean(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_minkowski'] = [minkowski(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_braycurtis'] = [braycurtis(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_chebyshev'] = [chebyshev(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_correlation'] = [correlation(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_hamming'] = [hamming(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_jaccard'] = [jaccard(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_kulsinski'] = [kulsinski(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_rogerstanimoto'] = [rogerstanimoto(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_russellrao'] = [russellrao(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_sokalmichener'] = [sokalmichener(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_kulsinski'] = [kulsinski(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        self.df['dist_yule'] = [yule(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "        #filling na values with 0  for cosine distance\n",
    "        self.df = self.df.fillna(0)\n",
    "        return self.df        \n",
    "\n",
    "    def wmd(self, s1, s2, model):\n",
    "        s1 = str(s1)\n",
    "        s2 = str(s2)\n",
    "        s1 = s1.split()\n",
    "        s2 = s2.split()\n",
    "        return model.wmdistance(s1, s2)\n",
    "    \n",
    "    def avg_w2v(self, list_of_sent, model, d):\n",
    "        '''\n",
    "        Returns average of word vectors for\n",
    "        each sentance with dimension of model given\n",
    "        '''\n",
    "        sent_vectors = []\n",
    "        for sent in list_of_sent: # for each review/sentence\n",
    "            doc = [word for word in sent if word in model.key_to_index]\n",
    "            if doc: \n",
    "                sent_vec = np.mean(model[doc],axis=0)\n",
    "            else:\n",
    "                sent_vec = np.zeros(d)\n",
    "            sent_vectors.append(sent_vec)\n",
    "        return sent_vectors\n",
    "       \n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "\n",
    "class Tfidf_Weighted_W2V_Features():\n",
    "    def __init__(self, df, w2v):\n",
    "        self.df = df\n",
    "        self.word2tfidf = w2v\n",
    "        \n",
    "    def extract_features(self):\n",
    "        df_new= pd.DataFrame()\n",
    "        df_new['id']=df['id']\n",
    "\n",
    "        df_new['q1_feats_m'] = list(self.tifidf_to_wieghtedW2V(self.df['question1'], self.word2tfidf))\n",
    "        df_new['q2_feats_m'] = list(self.tifidf_to_wieghtedW2V(self.df['question2'], self.word2tfidf))\n",
    "\n",
    "        df_q1 = pd.DataFrame(df_new['q1_feats_m'].values.tolist(), index= df_new.index)\n",
    "        df_q2 = pd.DataFrame(df_new['q2_feats_m'].values.tolist(), index= df_new.index)\n",
    "        df_q1['id']=df['id']\n",
    "        df_q2['id']=df['id']\n",
    "        result  = df_q1.merge(df_q2, on='id',how='left')\n",
    "        result['id']=df['id']\n",
    "\n",
    "        return self.df.merge(result, on='id',how='left')\n",
    "        \n",
    "    def tifidf_to_wieghtedW2V(self, text_df, tfidf_vectors):\n",
    "        vecs = []\n",
    "        # https://github.com/noamraph/tqdm\n",
    "        # tqdm is used to print the progress bar\n",
    "        for ques in tqdm(list(text_df)):\n",
    "            doc = nlp(ques) \n",
    "            # 384 is the number of dimensions of vectors \n",
    "            mean_vec = np.zeros([len(doc), len(doc[0].vector)])\n",
    "            for word in doc:\n",
    "                # word2vec\n",
    "                vec = word.vector\n",
    "                # fetch df score\n",
    "                try:\n",
    "                    idf = tfidf_vectors[str(word)]\n",
    "                except:\n",
    "                    idf = 0\n",
    "                # compute final vec\n",
    "                mean_vec += vec * idf\n",
    "            mean_vec = mean_vec.mean(axis=0)\n",
    "            vecs.append(mean_vec)\n",
    "        return vecs\n",
    "    \n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "\n",
    "class CleanFeature():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def clean_feature(self):\n",
    "        self.df = self.df.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], axis=1)\n",
    "        self.df = self.clean_dataset()\n",
    "        self.df = self.df.replace(np.nan, 0)\n",
    "        self.df.reset_index()\n",
    "        self.df = pd.DataFrame(np.array(self.df.values, dtype=np.float64), columns=list(self.df.columns))\n",
    "        return self.df\n",
    "\n",
    "    def clean_dataset(self):\n",
    "        assert isinstance(self.df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "        self.df.dropna(inplace=True)\n",
    "        indices_to_keep = ~self.df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "        return self.df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980582dc-3765-4633-9c30-25a3cf68d70b",
   "metadata": {},
   "source": [
    "## Applying Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e696f7-22d3-4eef-b612-b5650f134bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = LoadEssentials(glove_file=\"data/glove_vectors.txt\",\n",
    "                      word2tfidf_file=\"data/00_train.csv\",\n",
    "                      model_file=\"models/TfidfAvgW2V_XGBOOST.joblib\")\n",
    "glove, w2v, model = load.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53445b3c-f8ae-4590-bc09-5ec6004913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions_to_df(id, qid1, qid2,  q1, q2): \n",
    "    return pd.DataFrame([[id, qid1, qid2, q1, q2]], \n",
    "                        columns =[\"id\", 'qid1', 'qid2', 'question1', 'question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff66b648-d734-4596-9172-e7cda26ffc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.49106997, 0.50893   ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate question: 1\n",
    "dup_q1 = \"How do I read and find my YouTube comments?\"\n",
    "dup_q2 = \"Read my Youtube comments?\"\n",
    "\n",
    "df =  questions_to_df(4, 3, 5, dup_q1, dup_q2)\n",
    "df = BasicFeature(df).extract_features()\n",
    "df = AdvanceFeature(df).extract_features()\n",
    "df = DistanceFeature(df, glove).extract_features()\n",
    "df = Tfidf_Weighted_W2V_Features(df, w2v).extract_features()\n",
    "df = CleanFeature(df).clean_feature()\n",
    "print(model.predict(df))\n",
    "model.predict_proba(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f29575a6-e36e-4a54-844c-b957f57b7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 63.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5545331 , 0.44546688]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not duplicate question: \"0\"\n",
    "not_dup_q1 = \"What is the story of Kohinoor (Koh-i-Noor) Diamond?\"\n",
    "not_dup_q2 = \"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\"\n",
    "\n",
    "df =  questions_to_df(4, 3, 5, not_dup_q1, not_dup_q2)\n",
    "df = BasicFeature(df).extract_features()\n",
    "df = AdvanceFeature(df).extract_features()\n",
    "df = DistanceFeature(df, glove).extract_features()\n",
    "df = Tfidf_Weighted_W2V_Features(df, w2v).extract_features()\n",
    "df = CleanFeature(df).clean_feature()\n",
    "print(model.predict(df))\n",
    "model.predict_proba(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e94cb-7674-4657-8ea6-e857d99b8f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
